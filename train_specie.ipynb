{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2bfb14-aecd-4f7f-99a5-61c830bec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import seed_everything\n",
    "from data import ImageDataset, stratified_kfold, get_train_transforms, get_valid_transforms\n",
    "from model import SpecieClassifier\n",
    "from scheduler import CosineAnnealingWarmupRestarts\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65d0fe7-39de-426c-9184-3a8859536b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    checkpoint = '/root/dl_whale_classification/pths_specie'\n",
    "    test = False\n",
    "    tta = False\n",
    "    resume_epoch = 0\n",
    "    iters_to_accumulate = 1\n",
    "    resume_root = None\n",
    "    wandb_log = True\n",
    "    model_name = 'tf_efficientnet_b4_ns'\n",
    "    fold = 0\n",
    "    n_split = 5\n",
    "    seed = 2022\n",
    "    data_dir = '/root/data2/' # root/data/train_images\n",
    "    root_dir = '.'\n",
    "    batch_size = 16\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.0005\n",
    "    epoch = 20\n",
    "    exp_name = 'test'\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06571d4f-d3ad-491d-85d9-7e29a2cdc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e40b185-9dba-4f50-ab4c-aae2c79057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/data/train.csv')\n",
    "df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n",
    "                  \"pilot_whale\": \"short_finned_pilot_whale\",\n",
    "                  \"kiler_whale\": \"killer_whale\",\n",
    "                  \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7105f7-62c8-4452-8383-d3e4f4e35e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specie_unique = df.species.unique()\n",
    "specie_indices = range(len(specie_unique))\n",
    "species2idx = {k: v for k, v in zip(specie_unique, specie_indices)}\n",
    "df.species = df.species.map(species2idx)\n",
    "\n",
    "individual_unique = df.individual_id.unique()\n",
    "individual_indices = range(len(individual_unique))\n",
    "individual2idx = {k : v for k, v in zip(individual_unique, individual_indices)}\n",
    "df.individual_id = df.individual_id.map(individual2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8514e42-a2b2-4b03-9db3-b759ca21bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_species 26\n",
      "num_individual 15587\n"
     ]
    }
   ],
   "source": [
    "print('num_species', len(df.species.unique()))\n",
    "print('num_individual', len(df.individual_id.unique()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3443e37e-6393-41b0-9bd9-d8d9d92dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = df[df['individual_id'].map(df['individual_id'].value_counts()) == 1]\n",
    "df_others = df[df['individual_id'].map(df['individual_id'].value_counts()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69764e06-5613-4342-bd1c-2ea0a81d7902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_single, valid_single = stratified_kfold(df=df_single, fold=config.fold, n_split=config.n_split, seed=config.seed, target_col='species')\n",
    "train_others, valid_others = stratified_kfold(df=df_others, fold=config.fold, n_split=config.n_split, seed=config.seed, target_col='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c399e4-32ac-4e06-b5df-f75984a60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_indices = np.take(df_single.index.to_numpy(), train_single)    \n",
    "train_others_indices = np.take(df_others.index.to_numpy(), train_others)\n",
    "valid_single_indices = np.take(df_single.index.to_numpy(), valid_single)\n",
    "valid_others_indices = np.take(df_others.index.to_numpy(), valid_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1c2451-587e-43a6-adf8-8e13d96ebf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_indices = np.sort(np.concatenate((train_single_indices, train_others_indices), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc3fdd46-847f-4acc-b0d5-07ad92e54700",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames, labels = df['image'].values, df['species'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1714bad0-69e6-45a8-962e-dffc89f18a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_train, labels_train = fnames[full_train_indices], labels[full_train_indices]\n",
    "fnames_valid_single, labels_valid_single = fnames[valid_single_indices], labels[valid_single_indices]\n",
    "fnames_valid_others, labels_valid_others = fnames[valid_others_indices], labels[valid_others_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab3df49-e185-4e39-b2df-29d6ac843506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = get_train_transforms()\n",
    "valid_transforms = get_valid_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5f73e2-b89a-4271-a6ff-997e734de772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(path=fnames_train, target=labels_train, transform=train_transforms, root=config.data_dir + '/train_detec_512_v3/')\n",
    "valid_dataset_single = ImageDataset(path=fnames_valid_single, target=labels_valid_single, transform=valid_transforms, root=config.data_dir + '/train_detec_512_v3/')\n",
    "valid_dataset_others = ImageDataset(path=fnames_valid_others, target=labels_valid_others, transform=valid_transforms, root=config.data_dir + '/train_detec_512_v3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb8ed2e-9efc-439f-a9df-de843f2cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "valid_loader_single = DataLoader(valid_dataset_single, batch_size=config.batch_size*2, shuffle=False, num_workers=8, pin_memory=True)\n",
    "valid_loader_others = DataLoader(valid_dataset_others, batch_size=config.batch_size*2, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0cbe56b-0f00-4d85-ab78-936b282bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpecieClassifier(config.model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb8e343-3b5d-4014-9f07-d17449e70a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c713a4a-a0bd-41fa-95ca-fe06b49ebf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_annealing_scheduler_arg = dict(\n",
    "    first_cycle_steps=len(train_dataset)//config.batch_size*config.epoch,\n",
    "    cycle_mult=1.0,\n",
    "    max_lr=config.lr,\n",
    "    min_lr=1e-07,\n",
    "    warmup_steps=len(train_dataset)//config.batch_size*3, # wanrm up 0~3 epoch\n",
    "    gamma=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c175981-f637-4d08-916d-8f467b362d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, **cosine_annealing_scheduler_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d164bb3b-ea0e-473c-a6d2-21d02bd46207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcdata\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/dl_whale_classification/wandb/run-20220331_233400-1ak8vycp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jcdata/tf_efficientnet_b4_ns/runs/1ak8vycp\" target=\"_blank\">test_fold0</a></strong> to <a href=\"https://wandb.ai/jcdata/tf_efficientnet_b4_ns\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.wandb_log:\n",
    "    run = wandb.init(config=config.__dict__,\n",
    "                project=config.model_name, \n",
    "                settings=wandb.Settings(start_method=\"thread\"), \n",
    "                name=f\"{config.exp_name}_fold{config.fold}\",\n",
    "                reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f875601-e1c9-4ffe-bf3c-b01011f75850",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_model = None\n",
    "best_acc,  best_epoch = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc5d5c2-b1ad-4eca-988b-70c766f38132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.resume_root is not None:\n",
    "    check = torch.load(config.resume_root)\n",
    "    model.load_state_dict(check['model'])\n",
    "    optimizer.load_state_dict(check['optimizer'])\n",
    "    scheduler.load_state_dict(check['scheduler'])\n",
    "    print('loaded checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0f98d-5368-42e8-92b2-574f8dddbe62",
   "metadata": {},
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f0c62e-842f-49b5-b5bd-f45ece5a66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, loader, scheduler, scaler=None, iters_to_accumulate=1):\n",
    "    model.train()\n",
    "    \n",
    "    match = 0\n",
    "    top_k_match = 0\n",
    "    \n",
    "    losses, y_true, y_pred = [], [], []\n",
    "    for i, (x, y) in enumerate(tqdm(loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(x) \n",
    "                loss = criterion(output, y)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if ((i + 1) % iters_to_accumulate == 0) or ((i + 1) == len(loader)):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        else:\n",
    "            output = model(x, y) \n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        match += (torch.argmax(output.detach().cpu(), dim=-1) == y.detach().cpu()).sum()\n",
    "        top_k_match += (output.detach().topk(2, dim=-1).indices.cpu() == y.detach().cpu()[:, None]).sum()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "        \n",
    "    return np.mean(losses), match, top_k_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1295dffa-6bd4-4400-b7d5-6cbe2d8aa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(model, criterion, loader, tta=False):\n",
    "    model.eval()\n",
    "    \n",
    "    match = 0\n",
    "    top_k_match = 0\n",
    "\n",
    "    losses, y_trues, y_preds, tta_embed = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in tqdm(enumerate(loader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(x) \n",
    "                loss = criterion(output, y)\n",
    "  \n",
    "            match += (torch.argmax(output.detach().cpu(), dim=-1) == y.detach().cpu()).sum()\n",
    "            top_k_match += (output.detach().topk(2, dim=-1).indices.cpu() == y.detach().cpu()[:, None]).sum()\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            \n",
    "            y_preds.extend(np.argmax(output.detach().cpu().numpy(), axis=-1))\n",
    "            y_trues.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "    return np.mean(losses), match, top_k_match, np.array(y_preds), np.array(y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a645c01-6050-4ed7-9b91-abcbe45121a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64ac3fb1-cb07-4a61-aeac-ef7f72fbcc96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18062a34b0944d19b5fd767d78f2155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m train_loss, train_acc, train_top_k \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miters_to_accumulate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m valid_loss1, valid_acc1, valid_top_k1, valid_preds1, valid_trues1 \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, criterion, valid_loader_single, config\u001b[38;5;241m.\u001b[39mtta)\n\u001b[1;32m     10\u001b[0m valid_loss2, valid_acc2, valid_top_k2, valid_preds2, valid_trues2 \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, criterion, valid_loader_others, config\u001b[38;5;241m.\u001b[39mtta)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, criterion, loader, scheduler, scaler, iters_to_accumulate)\u001b[0m\n\u001b[1;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(x) \n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m iters_to_accumulate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)):\n\u001b[1;32m     19\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "os.makedirs(f\"{config.checkpoint}/{config.model_name}\", exist_ok=True)\n",
    "\n",
    "print('Start Training')\n",
    "for epo in range(config.resume_epoch, config.epoch):\n",
    "    print(f\"epoch: {epo}\")\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    train_loss, train_acc, train_top_k = train_one_epoch(model, optimizer, criterion, train_loader, scheduler, grad_scaler, config.iters_to_accumulate)\n",
    "    valid_loss1, valid_acc1, valid_top_k1, valid_preds1, valid_trues1 = valid_one_epoch(model, criterion, valid_loader_single, config.tta)\n",
    "    valid_loss2, valid_acc2, valid_top_k2, valid_preds2, valid_trues2 = valid_one_epoch(model, criterion, valid_loader_others, config.tta)\n",
    "\n",
    "    print(f\"train loss {train_loss :.4f} acc {train_acc :.4f} topk {train_top_k :.4f}\")\n",
    "    print(f\"valid loss (single) {valid_loss1 :.4f} acc {valid_acc1/len(valid_loader_single.dataset) :.4f} topk {valid_top_k1/len(valid_loader_single.dataset) :.4f}\")\n",
    "    print(f\"valid loss (others) {valid_loss2 :.4f} acc {valid_acc2/len(valid_loader_others.dataset) :.4f} topk {valid_top_k2/len(valid_loader_others.dataset) :.4f}\")\n",
    "    print(f\"lr {lr} time {time.time() - start :.2f}s\")\n",
    "    \n",
    "    print(\"-\"*20, f\"class (single)\", \"-\"*20)\n",
    "    for i in range(26):\n",
    "        print(f\"{i:02d}-#{(valid_trues1==i).sum():04d} : {((valid_preds1==i)&(valid_trues1==i)).sum()/(valid_trues1==i).sum():04f}\", end=' / ') \n",
    "        if i % 4 == 3 and i > 0 or i == 25:\n",
    "            print()\n",
    "\n",
    "    print(\"-\"*20, f\"class (others)\", \"-\"*20)\n",
    "    for i in range(26):\n",
    "        print(f\"{i:02d}-#{(valid_trues2==i).sum():04d} : {((valid_preds2==i)&(valid_trues2==i)).sum()/(valid_trues2==i).sum():04f}\", end=' / ') \n",
    "        if i % 4 == 3 and i > 0 or i == 25:\n",
    "            print()\n",
    "                                     \n",
    "    if best_acc < valid_acc2/len(valid_loader_others.dataset):\n",
    "        best_acc = valid_acc2/len(valid_loader_others.dataset)\n",
    "        best_epoch = epo\n",
    "        print(f'best acc updated {best_acc}')\n",
    "        best_model_dict = {\n",
    "            'model': deepcopy(model.state_dict()),\n",
    "            'optimizer': deepcopy(optimizer.state_dict()),\n",
    "            'scheduler': deepcopy(scheduler.state_dict()),\n",
    "        }\n",
    "        torch.save(best_model_dict, f\"{config.checkpoint}/{config.model_name}/best.pt\")\n",
    "\n",
    "    if config.wandb_log:\n",
    "        wandb_dict = {\n",
    "            'train loss': train_loss,\n",
    "            'train acc': train_acc,\n",
    "            'valid loss (single)': valid_loss1,\n",
    "            'valid acc (single)': valid_acc1 / len(valid_loader_single.dataset),\n",
    "            'valid topk (single)': valid_top_k1 / len(valid_loader_single.dataset),\n",
    "            'valid loss (others)': valid_loss2,\n",
    "            'valid acc (others)': valid_acc2 / len(valid_loader_others.dataset),\n",
    "            'valid topk (others)': valid_top_k2 / len(valid_loader_others.dataset),\n",
    "            'learning rate': scheduler.get_lr()[0],\n",
    "        }\n",
    "        wandb.log(wandb_dict)\n",
    "        \n",
    "os.rename(f\"{config.checkpoint}/{config.model_name}/best.pt\", \\\n",
    "          f\"{config.checkpoint}/{config.model_name}/fold{config.fold}_epoch{best_epoch}_{best_acc:.04f}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84f5652a-72e6-49f0-aeb6-86b7bc18e489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_trues1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff39ee-7ffb-4dea-bf7f-42667bf36320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad83ae2-acd5-41f0-96a2-423197e39543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a74ab-fa43-4e6d-8928-02aad2a38968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
