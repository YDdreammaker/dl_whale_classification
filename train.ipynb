{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2bfb14-aecd-4f7f-99a5-61c830bec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import seed_everything\n",
    "from data import ImageDataset, stratified_kfold, get_train_transforms, get_valid_transforms\n",
    "from model import SpecieClassifier\n",
    "from scheduler import CosineAnnealingWarmupRestarts\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65d0fe7-39de-426c-9184-3a8859536b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    checkpoint = '/root/dl_whale_classification/pths'\n",
    "    test = False\n",
    "    tta = False\n",
    "    resume_epoch = 0\n",
    "    iters_to_accumulate = 1\n",
    "    resume_root = None\n",
    "    wandb_log = True\n",
    "    model_name = 'tf_efficientnet_b3_ns'\n",
    "    fold = 0\n",
    "    n_split = 5\n",
    "    seed = 2022\n",
    "    data_dir = '/root/data/' # root/data/train_images\n",
    "    root_dir = '.'\n",
    "    batch_size = 16\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.0005\n",
    "    epoch = 10\n",
    "    exp_name = 'test'\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06571d4f-d3ad-491d-85d9-7e29a2cdc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e40b185-9dba-4f50-ab4c-aae2c79057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.data_dir + 'train.csv')\n",
    "df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n",
    "                  \"pilot_whale\": \"short_finned_pilot_whale\",\n",
    "                  \"kiler_whale\": \"killer_whale\",\n",
    "                  \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7105f7-62c8-4452-8383-d3e4f4e35e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specie_unique = df.species.unique()\n",
    "specie_indices = range(len(specie_unique))\n",
    "species2idx = {k: v for k, v in zip(specie_unique, specie_indices)}\n",
    "df.species = df.species.map(species2idx)\n",
    "\n",
    "individual_unique = df.individual_id.unique()\n",
    "individual_indices = range(len(individual_unique))\n",
    "individual2idx = {k : v for k, v in zip(individual_unique, individual_indices)}\n",
    "df.individual_id = df.individual_id.map(individual2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8514e42-a2b2-4b03-9db3-b759ca21bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_species 26\n",
      "num_individual 15587\n"
     ]
    }
   ],
   "source": [
    "print('num_species', len(df.species.unique()))\n",
    "print('num_individual', len(df.individual_id.unique()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3443e37e-6393-41b0-9bd9-d8d9d92dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = df[df['individual_id'].map(df['individual_id'].value_counts()) == 1]\n",
    "df_others = df[df['individual_id'].map(df['individual_id'].value_counts()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69764e06-5613-4342-bd1c-2ea0a81d7902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_single, valid_single = stratified_kfold(df=df_single, fold=config.fold, n_split=config.n_split, seed=config.seed, target_col='species')\n",
    "train_others, valid_others = stratified_kfold(df=df_others, fold=config.fold, n_split=config.n_split, seed=config.seed, target_col='individual_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c399e4-32ac-4e06-b5df-f75984a60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_indices = np.take(df_single.index.to_numpy(), train_single)    \n",
    "train_others_indices = np.take(df_others.index.to_numpy(), train_others)\n",
    "valid_single_indices = np.take(df_others.index.to_numpy(), valid_single)\n",
    "valid_others_indices = np.take(df_others.index.to_numpy(), valid_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1c2451-587e-43a6-adf8-8e13d96ebf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_indices = np.sort(np.concatenate((train_single_indices, train_others_indices), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc3fdd46-847f-4acc-b0d5-07ad92e54700",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames, labels = df['image'].values, df['species'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1714bad0-69e6-45a8-962e-dffc89f18a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_train, labels_train = fnames[full_train_indices], labels[full_train_indices]\n",
    "fnames_valid_single, labels_valid_single = fnames[valid_single_indices], labels[valid_single_indices]\n",
    "fnames_valid_others, labels_valid_others = fnames[valid_others_indices], labels[valid_others_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab3df49-e185-4e39-b2df-29d6ac843506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = get_train_transforms()\n",
    "valid_transforms = get_valid_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5f73e2-b89a-4271-a6ff-997e734de772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(path=fnames_train, target=labels_train, transform=train_transforms, root=config.data_dir + '/train_images/')\n",
    "valid_dataset_single = ImageDataset(path=fnames_valid_single, target=labels_valid_single, transform=valid_transforms, root=config.data_dir + '/train_images/')\n",
    "valid_dataset_others = ImageDataset(path=fnames_valid_others, target=labels_valid_others, transform=valid_transforms, root=config.data_dir + '/train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb8ed2e-9efc-439f-a9df-de843f2cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "valid_loader_single = DataLoader(valid_dataset_single, batch_size=config.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "valid_loader_others = DataLoader(valid_dataset_others, batch_size=config.batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0cbe56b-0f00-4d85-ab78-936b282bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpecieClassifier('tf_efficientnet_b3_ns').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb8e343-3b5d-4014-9f07-d17449e70a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c713a4a-a0bd-41fa-95ca-fe06b49ebf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_annealing_scheduler_arg = dict(\n",
    "    first_cycle_steps=len(train_dataset)//config.batch_size*config.epoch,\n",
    "    cycle_mult=1.0,\n",
    "    max_lr=config.lr,\n",
    "    min_lr=1e-07,\n",
    "    warmup_steps=len(train_dataset)//config.batch_size*3, # wanrm up 0~3 epoch\n",
    "    gamma=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c175981-f637-4d08-916d-8f467b362d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, **cosine_annealing_scheduler_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d164bb3b-ea0e-473c-a6d2-21d02bd46207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maperyear\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/dl_whale_classification/wandb/run-20220325_143738-mu50zswl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aperyear/tf_efficientnet_b3_ns/runs/mu50zswl\" target=\"_blank\">test_fold0</a></strong> to <a href=\"https://wandb.ai/aperyear/tf_efficientnet_b3_ns\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.wandb_log:\n",
    "    run = wandb.init(config=config.__dict__,\n",
    "                project=config.model_name, \n",
    "                settings=wandb.Settings(start_method=\"thread\"), \n",
    "                name=f\"{config.exp_name}_fold{config.fold}\",\n",
    "                reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f875601-e1c9-4ffe-bf3c-b01011f75850",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_model = None\n",
    "best_acc,  best_epoch = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc5d5c2-b1ad-4eca-988b-70c766f38132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.resume_root is not None:\n",
    "    check = torch.load(config.resume_root)\n",
    "    model.load_state_dict(check['model'])\n",
    "    optimizer.load_state_dict(check['optimizer'])\n",
    "    scheduler.load_state_dict(check['scheduler'])\n",
    "    print('loaded checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0f98d-5368-42e8-92b2-574f8dddbe62",
   "metadata": {},
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f0c62e-842f-49b5-b5bd-f45ece5a66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, loader, scheduler, scaler=None, iters_to_accumulate=1):\n",
    "    model.train()\n",
    "    \n",
    "    match = 0\n",
    "    top_k_match = 0\n",
    "    \n",
    "    losses, y_true, y_pred, embed = [], [], [], []\n",
    "    for i, (x, y) in enumerate(tqdm(loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(x) \n",
    "                loss = criterion(output, y)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if ((i + 1) % iters_to_accumulate == 0) or ((i + 1) == len(loader)):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        else:\n",
    "            output = model(x, y) \n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        match += (torch.argmax(output.detach().cpu(), dim=-1) == y.detach().cpu()).sum()\n",
    "        top_k_match += (output.detach().topk(2, dim=-1).indices.cpu() == y.detach().cpu()[:, None]).sum()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        if i == 100:\n",
    "            break\n",
    "        \n",
    "    return np.mean(losses), match, top_k_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1295dffa-6bd4-4400-b7d5-6cbe2d8aa532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7f76b5149040> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:328\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:581\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:279\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7f76b5149430> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:323\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mlog_code(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, res)\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:573\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:275\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "def valid_one_epoch(model, criterion, loader, tta=False):\n",
    "    model.eval()\n",
    "    \n",
    "    match = 0\n",
    "    top_k_match = 0\n",
    "\n",
    "    losses, y_true, y_pred, embed, tta_embed = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in tqdm(enumerate(loader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            output = model(x) \n",
    "\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            y_true.extend(y.detach().cpu())\n",
    "            y_pred.extend(output.detach().cpu().topk(2, dim=-1).indices)   \n",
    "            match += (torch.argmax(output.detach().cpu(), dim=-1) == y.detach().cpu()).sum()\n",
    "            top_k_match += (output.detach().cpu().topk(2, dim=-1).indices == y.detach().cpu()[:, None]).sum()\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            \n",
    "\n",
    "    return np.mean(losses), match, top_k_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a645c01-6050-4ed7-9b91-abcbe45121a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac3fb1-cb07-4a61-aeac-ef7f72fbcc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7f76b5149040> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:328\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:581\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:279\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b134fa536e940389f80d3d265308548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "print('Start Training')\n",
    "for i in range(config.resume_epoch, config.epoch):\n",
    "    print(f\"epoch: {i}\")\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    train_loss, train_acc, train_top_k = train_one_epoch(model, optimizer, criterion, train_loader, scheduler, grad_scaler, config.iters_to_accumulate)\n",
    "    valid_loss1, valid_acc1, valid_top_k1 = valid_one_epoch(model, criterion, valid_loader_single, config.tta)\n",
    "    valid_loss2, valid_acc2, valid_top_k2 = valid_one_epoch(model, criterion, valid_loader_others, config.tta)\n",
    "\n",
    "    print(f\"train loss {train_loss :.4f} acc {train_acc :.4f} topk {train_top_k :.4f}\")\n",
    "    print(f\"valid loss (single) {valid_loss1 :.4f} acc {valid_acc1/len(valid_loader_single.dataset) :.4f} topk {valid_top_k1/len(valid_loader_single.dataset) :.4f}\")\n",
    "    print(f\"valid loss (others) {valid_loss2 :.4f} acc {valid_acc2/len(valid_loader_others.dataset) :.4f} topk {valid_top_k2/len(valid_loader_others.dataset) :.4f}\")\n",
    "    print(f\"lr {lr} time {time.time() - start :.2f}s\")\n",
    "\n",
    "    if best_acc < valid_acc1:\n",
    "        best_acc = valid_acc1\n",
    "        best_epoch = i\n",
    "        print(f'best acc updated {best_acc}')\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "\n",
    "    if config.wandb_log:\n",
    "        wandb_dict = {\n",
    "            'train loss': train_loss,\n",
    "            'train acc': train_acc,\n",
    "            'valid loss (single)': valid_loss1,\n",
    "            'valid acc (single)': valid_acc1 / len(valid_loader_single.dataset),\n",
    "            'valid topk (single)': valid_top_k1 / len(valid_loader_single.dataset),\n",
    "            'valid loss (others)': valid_loss2,\n",
    "            'valid acc (others)': valid_acc2 / len(valid_loader_others.dataset),\n",
    "            'valid topk (others)': valid_top_k2 / len(valid_loader_others.dataset),\n",
    "            'learning rate': scheduler.get_lr()[0],\n",
    "        }\n",
    "        wandb.log(wandb_dict)\n",
    "\n",
    "    if not config.test:\n",
    "        check_dict = {\n",
    "            'model': deepcopy(model.state_dict()),\n",
    "            'optimizer': deepcopy(optimizer.state_dict()),\n",
    "            'scheduler': deepcopy(scheduler.state_dict()),\n",
    "        }\n",
    "        \n",
    "        os.makedirs(f\"{config.checkpoint}/{config.model_name}\", exist_ok=True)\n",
    "        torch.save(check_dict, f\"{config.checkpoint}/{config.model_name}/fold{config.fold}_epoch{i}_{config.exp_name}_{best_acc}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff39ee-7ffb-4dea-bf7f-42667bf36320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad83ae2-acd5-41f0-96a2-423197e39543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a74ab-fa43-4e6d-8928-02aad2a38968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
